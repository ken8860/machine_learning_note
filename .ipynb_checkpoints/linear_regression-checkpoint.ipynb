{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495558fb-5977-4ee1-92be-1e6837ff050d",
   "metadata": {},
   "source": [
    "# 1、线性回归\n",
    "### 定义：线性回归是一种用于建立变量之间线性关系的统计模型。它试图找到一个线性函数，使得该函数能够最好地描述自变量（解释变量）与因变量（响应变量）之间的关系，从而可以根据自变量的值来预测或解释因变量的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f149deb3-7443-4a18-87de-ceabe3be10d1",
   "metadata": {},
   "source": [
    "# 2、线性回归主要数学公式\n",
    "### 线性回归主要是将训练数据拟合成一个多元线性函数，若需要拟合的数据有n个特征属性值，有m个数据，一个标签，那么线性回归就将其拟合成一个n元线性函数$h_\\theta(x_0^i,x_1^i,x_2^i,...,x_j^i)=h_\\theta(\\vec{x^i})=(\\vec{\\theta})^T\\cdot\\vec{x^i}$(其中$\\vec{x^i}=(x^i_0,x^i_1,x^i_2,x^i_3,...,x^i_j)^T,\\vec{\\theta}=(\\theta_0,\\theta_1,\\theta_2,\\theta_3,...,\\theta_j)^T$，i表示第i个数据，总共有m个，i=1,2,3,...,m，j表示第j个特征值，总共有n个，j=0,1,2,3,...,n,其中第0个特征认为是线性函数的常数项，故$x_0^i$恒等于1)，比如：现有三个属性：房子面积$x_1$，房子楼层$x_2$，房子地段$x_3$。一个标签：房子价格$y$。那么线性回归拟合的线性函数为：$y=\\theta_1x_1+\\theta_2x_2+\\theta_3x_3+\\theta_0$，其中$\\theta_0,\\theta_1,\\theta_2,\\theta_3$为拟合的目标，通过梯度下降找到最佳的$\\theta_j$。\n",
    "### 使用梯度下降进行优化损失函数，使得损失函数最小:\n",
    "#### 目标损失函数：$J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(y^i-h_\\theta(\\vec{x^i}))^2$\n",
    "#### 梯度下降公式：$\\frac{\\partial J(\\theta)}{\\partial \\theta_j}=-\\frac{1}{m}\\sum_{i=1}^{m}(y^i-h_\\theta(\\vec{x^i}))x^i_j$\n",
    "#### 批量梯度下降：随机找到一个初始$\\theta_j$(j是每个特征对应的j，如$x_1$对应的j是1)然后逐步更新$\\theta_j$,根据以下公式$\\theta^{'}_j=\\theta_j-\\frac{\\partial J(\\theta)}{\\partial \\theta_j}$直到找到最小的$\\theta_j$(每一个$\\theta_j$同时更新)\n",
    "\n",
    "#### 随机梯度下降：随机找到一个初始$\\theta_j$(j是每个属性对应的j，如$x_1$对应的j是1)然后逐步更新$\\theta_j$,根据以下公式$\\theta^{'}_j=\\theta_j+(y^i-h_\\theta(x^i))x_j^i$直到找到最小的$\\theta_j$(每一个$\\theta_j$同时更新)\n",
    "\n",
    "#### 小批量梯度下降（一般用这个）:随机找到一个初始$\\theta_j$(j是每个属性对应的j，如$x_1$对应的j是1)然后逐步更新$\\theta_j$,根据以下公式$\\theta^{'}_j=\\theta_j-\\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_j}$($\\alpha$为学习率)直到找到最小的$\\theta_j$(每一个$\\theta_j$同时更新)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838bf68-aeca-419e-8604-ff600a85a3fc",
   "metadata": {},
   "source": [
    "# 3、真实数据展示\n",
    "### 数据说明：五组数据，每组数据有4个特征值：\n",
    "|特征值1|特征值2|特征值3|特征值4|标签|\n",
    "|---|---|---|---|---|\n",
    "|1.2|2.5|3.7|4.1|12.3|\n",
    "|2.1|3.2|4.5|5.3|16.8|\n",
    "|3.3|4.7|5.9|6.2|20.5|\n",
    "|4.2|5.1|6.7|7.3|24.8|\n",
    "|5.4|6.3|7.2|8.1|29.6|\n",
    "### 以小批量梯度下降为准，特征值1，特征值2，特征值3，特征值4为$x_1,x_2,x_3,x_4$，标签为$y$，目标：找到一个函数：$y=\\theta_1x_1+\\theta_2x_2+\\theta_3x_3+\\theta_4x_4+\\theta_0$，于是假设初始$\\theta_1=1,\\theta_2=1,\\theta_3=1,\\theta_4=1,\\theta_0=1,\\alpha=0.1$，于是，以更新$\\theta_1$为例，$\\frac{\\partial J(\\theta)}{\\partial \\theta_1}=-\\frac{1}{m}\\sum_{i=1}^{m}(y^i-h_\\theta(x^i))x^i_1=-\\frac{1}{5}((12.3-(1\\times1.2+1\\times2.5+1\\times3.7+1\\times4.1+1))\\times1.2+(16.8-(1\\times2.1+1\\times3.2+1\\times4.5+1\\times5.3))\\times2.1+(20.5-(1\\times3.3+1\\times4.7+1\\times5.9+1\\times6.2))\\times3.3+(24.8-(1\\times4.2+1\\times5.1+1\\times6.7+1\\times7.3))\\times4.2+(29.6-(1\\times5.4+1\\times6.3+1\\times7.2+1\\times8.1))\\times5.4)=-1.998$，按照小批量梯度下降公式可得$\\theta^{'}_1=\\theta_1-\\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_1}=1-0.1\\times(-1.998)=1.1998$，同时，按照此$\\theta_1$更新，更新$\\theta_0,\\theta_2,\\theta_3,\\theta_4$（在更新$\\theta_0$时,将所有$x^i_j=1$），于是，就得到了第一次更新的$\\theta_0,\\theta_1,\\theta_2,\\theta_3,\\theta_4$，接下来，更新n次，使得$J(\\theta)$最小。\n",
    "### 代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b740e439-7458-4a9a-8efc-7336b34c1d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.11157407 -0.15694444 -1.44444444  2.47083333]\n",
      "[4.1725]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.array([[1.2,2.5,3.7,4.1],\n",
    "              [2.1,3.2,4.5,5.3],\n",
    "              [3.3,4.7,5.9,6.2],\n",
    "              [4.2,5.1,6.7,7.3],\n",
    "              [5.4,6.3,7.2,8.1]])\n",
    "y = np.array([[12.3],[16.8],[20.5],[24.8],[29.6]])\n",
    "reg = LinearRegression()\n",
    "reg.fit(X,y)\n",
    "w_ =np.array(reg.coef_)\n",
    "w = w_.flatten()\n",
    "b = np.array(reg.intercept_)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1f541-a2ba-4eae-b862-5c707bc50e10",
   "metadata": {},
   "source": [
    "# 4、多项式回归\n",
    "### 定义：多项式线性回归是线性回归的一种扩展，通过引入特征的高次项（如$x^2$,$x^3$,...）来拟合非线性关系。虽然输入特征被非线性变换，但模型对参数$\\theta$仍然是线性的，即将原本的n个属性通过两两组合的方式变成$\\sum_{i=1}^{i=k}C_{n+i-1}^i$个属性，其中k扩展的最高次项的次数，比如：有三个属性：$x_1,x_2,x_3$,将其扩展到最高次项的次数为2，那么，它就变成$C_{n}^1+C_{n+1}^2=C_{3}^1+C_{3+1}^2=9$项，分别为：$x_1,x_2,x_3,x_1^2,x_2^2,x_3^2,x_1x_2,x_1x_3,x_2x_3$，最后把这9项当成9个属性作线性回归，就得到了多项式线性回归的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abaf98c-c58c-4161-a036-db2cfe3682a7",
   "metadata": {},
   "source": [
    "# 5、正则化\n",
    "### 1）lasso回归：\n",
    "#### Lasso 回归，即最小绝对收缩和选择算子回归（Least Absolute Shrinkage and Selection Operator），是一种用于线性回归模型的正则化方法，为的是防止过拟合，即回归曲线完美拟合训练数据，但是对测试数据效果预测效果不理想的情况。lasso回归增加了惩罚项：$\\lambda\\sum_{j=1}^{n}\\left|\\theta_j\\right|$（n表示特征数量）。于是，目标损失函数就变成：$J^{'}(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(y^i-h_\\theta(\\vec{x^i}))^2+\\lambda\\sum_{j=1}^{n}\\left|\\theta_j\\right|$，对于该损失函数的优化，使用坐标轴下降法(coordinate descent)优化$\\theta_0$不需要正则优化，步骤如下：\n",
    "#### 1.把目标损失函数展开: $J^{'}(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(y^i-h_\\theta(\\vec{x^i}))^2+\\lambda\\sum_{j=1}^{n}\\left|\\theta_j\\right|=\\frac{1}{2m}\\sum_{i=1}^{m}(\\sum_{j=1}^{n}\\theta_jx^i_j+\\theta_0-y^i)^2+\\lambda\\sum_{j=1}^{n}\\left|\\theta_j\\right|$\n",
    "#### 2.固定其他$\\theta_j$，先优化其中的$\\theta_l$，$l=1,2,3,4,...,n$求其对于$\\theta_l$的偏导数: $\\frac{\\partial J(\\theta)}{\\partial \\theta_l}=\\frac{1}{m}\\sum_{i=1}^{m}(\\sum_{j=1}^{n}\\theta_jx^i_j+\\theta_0-y^i)x^i_l+\\lambda\\frac{\\partial\\left|\\theta_l\\right|}{\\partial\\theta_l}=\\frac{1}{m}\\sum_{i=1}^{m}(\\sum_{j=1,j\\neq l}^{n}\\theta_jx^i_j+\\theta_0-y^i+\\theta_lx^i_l)x^i_l+\\lambda\\frac{\\partial\\left|\\theta_l\\right|}{\\partial\\theta_l}=\\frac{1}{m}\\sum_{i=1}^{m}(\\sum_{j=1,j\\neq l}^{n}\\theta_jx^i_j+\\theta_0-y^i)+\\frac{1}{m}\\sum_{i=1}^{m}\\theta_l(x^i_l)^2+\\lambda\\frac{\\partial\\left|\\theta_l\\right|}{\\partial\\theta_l}$(正则项 $\\lambda\\frac{\\partial\\left|\\theta_l\\right|}{\\partial\\theta_l}$ 需要分情况讨论)，由于先固定除$\\theta_l$之外的其他$\\theta_j$故把第一项$\\frac{1}{m}\\sum_{i=1}^{m}(\\sum_{j=1,j\\neq l}^{n}\\theta_jx^i_j+\\theta_0-y^i)$当作常数，作记号为$C_l$，第二项$\\frac{1}{m}\\sum_{i=1}^{m}\\theta_l(x^i_l)^2$是优化项，将$\\frac{1}{m}\\sum_{i=1}^{m}(x^i_l)^2$ 作记号为 $A_l$，显然 $A_l>0$，一般情况下不太可能等于0，于是认为它是大于0的，于是原式就变$\\frac{\\partial J(\\theta)}{\\partial \\theta_l}=C_l+A_l\\theta_l+\\lambda\\frac{\\partial\\left|\\theta_l\\right|}{\\partial\\theta_l}$\n",
    "#### 3.分情况讨论$\\frac{\\partial J(\\theta)}{\\partial \\theta_l}$: \n",
    "##### *a.当$\\theta_l>0$时，$\\frac{\\partial J(\\theta)}{\\partial \\theta_l}=C_l+A_l\\theta_l+\\lambda$ ($\\left|\\theta_l\\right|=\\theta_l$)*\n",
    "##### *b.当$\\theta_l<0$时，$\\frac{\\partial J(\\theta)}{\\partial \\theta_l}=C_l+A_l\\theta_l-\\lambda$ ($\\left|\\theta_l\\right|=-\\theta_l$)*\n",
    "##### *c.当$\\theta_l=0$时，$\\frac{\\partial J(\\theta)}{\\partial \\theta_l}\\in [C_l+A_l\\theta_l-\\lambda,C_l+A_l\\theta_l+\\lambda]$ ($\\left.\\frac{\\partial\\left|\\theta_l\\right|}{\\partial \\theta_l}\\right|_{\\theta_l\\to0^-}=C_l+A_l\\theta_l-\\lambda,\\left.\\frac{\\partial\\left|\\theta_l\\right|}{\\partial \\theta_l}\\right|_{\\theta_l\\to0^+}=C_l+A_l\\theta_l+\\lambda$)*\n",
    "#### 4.令偏导数$\\frac{\\partial J(\\theta)}{\\partial \\theta_l}=0$ 解得:\n",
    "##### *a.当$\\theta_l>0$时，$\\theta_l=\\frac{-C_l-\\lambda}{A_l}>0$因为 $A_l>0$ 故 $-C_l-\\lambda>0$，即 $C_l<-\\lambda$*\n",
    "##### *b.当$\\theta_l<0$时，$\\theta_l=\\frac{\\lambda-C_l}{A_l}<0$因为 $A_l>0$ 故 $\\lambda-C_l<0$，即 $C_l>\\lambda$*\n",
    "##### *c.当$\\theta_l=0$时，$C_l\\in [-\\lambda,\\lambda]$*\n",
    "#### 5.最终$\\theta_l$更新的结论:\n",
    "##### *a.当$C_l<-\\lambda$时，$\\theta_l=\\frac{-C_l-\\lambda}{A_l}$*\n",
    "##### *b.当$C_l>\\lambda$时，$\\theta_l=\\frac{\\lambda-C_l}{A_l}$*\n",
    "##### *c.当$C_l\\in [-\\lambda,\\lambda]$时，$\\theta_l=0$*\n",
    "#### 6.最终所有的$\\theta_j$更新: 将$l$取$1,2,3,4,...,n$，每次取$l$为其中一个值，固定其他值，直至所有的除$\\theta_0$外的$\\theta_j$都更新，最后就得到最优解\n",
    "\n",
    "#### 7.对于$\\theta_0$的更新:因为正则项里面$\\theta_j$中$j$是从$1$到$n$，所以更新$\\theta_0$的方法与没有正则项的目标损失函数一样的优化。\n",
    "\n",
    "### 2）岭回归：\n",
    "#### 岭回归（Ridge Regression）是一种用于处理线性回归模型中多重共线性问题的有偏估计方法，与lasso回归一样，都是为了防止过拟合，于是增加了惩罚项：$\\frac{\\lambda}{2}\\sum_{j=1}^{n}\\theta_j^2$（n表示特征数量）。于是，目标损失函数就变成：$J^{'}(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(y^i-h_\\theta(\\vec{x^i}))^2+\\frac{\\lambda}{2}\\sum_{j=1}^{n}\\theta_j^2$，想要优化损失函数，使用梯度下降法找到最优的$\\vec{\\theta}=(\\theta_0,\\theta_1,\\theta_2,...,\\theta_n)$，其中$\\theta_0$不需要正则优化。于是，使用小批量梯度下降，$\\theta^{'}_j=\\theta_j-\\alpha\\frac{\\partial J^{'}(\\theta)}{\\partial \\theta_j}=\\theta_j-\\alpha(-\\frac{1}{m}\\sum_{i=1}^{m}(y^i-h_\\theta(\\vec{x^i}))x^i_j+\\lambda\\theta_j)=\\theta_j-\\alpha(\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(\\vec{x^i})-y^i))x^i_j+\\lambda\\theta_j)=(1-\\alpha\\lambda)\\theta_j-\\alpha(\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(\\vec{x^i})-y^i))x^i_j)=(1-\\alpha\\lambda)\\theta_j-\\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_j}$，其中$1-\\alpha\\lambda$ 是正则项。对于$\\theta_0$的更新，因为正则项里面$\\theta_j$中$j$是从$1$到$n$，所以更新$\\theta_0$的方法与没有正则项的目标损失函数一样的优化。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
