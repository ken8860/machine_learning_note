{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37c636c-dbfb-4945-ae66-e30a8a8e0c5a",
   "metadata": {},
   "source": [
    "# 1. 逻辑回归的定义\n",
    "### 1）问题引入：对于普通的线性回归模型，可以实现回归任务，例如对房价的预测等，但是，如果想要将其变成了一个分类问题（将数据按照其特征将其分为A类，B类，C类等）。先讨论二分类（只有A，B两类），那么现在就有一个问题：对于普通的线性回归预测，它的预测范围可以从负无穷到正无穷，但是对于二分类问题，他的标签（输出值）为$y^{(i)}=\\left\\{0,1\\right\\}$（其中$0$代表A类，$1$代表B类），显然不能用简单的线性回归解决问题，因为它的输出是离散值。\n",
    "### 2）sigmoid 函数的引入：想要解决这个问题，就必须要让其输出值变为连续值，显然，最好刻画分类问题的是概率，因为概率从$0$到$1$之间是连续值，同样它也是一个很好的映射，如当数据将数据带入模型的时候，如果输出分类为A的概率为$p$，输出分类为B的概率为$1-p$（如果是多分类问题就是$p_1$,$p_2$,$p_3$,...,$p_n$，它们的和为$1$）,那么如果$p>0.5$说明这个模型预测出该数据分类为A的概率更大，于是将其分类为A,否则分类为B，当$p=0.5$的时候，说明这个模型非常糟糕，不适合再使用，因为如果人工选择一个数据分类为A或者B的概率就是50%分对，不需要构建这样一个模型。因此，解决这个问题就是要引入一个函数，使得线性模型输出在负无穷到正无穷的情况下，映射到$0$到$1$，于是就有sigmoid函数：$h(x)=\\frac{1}{1+e^{-x}}$，观察这个函数$\\lim_{x\\to+\\infty}h(x)=1$，$\\lim_{x\\to-\\infty}h(x)=0$，$h^{'}(x)=\\frac{e^{-x}}{(1+e^{-x})^2}>0$这个函数在负无穷到正无穷都有定义，且是单调递增的，所以他是连续的，刚好满足映射函数的需求，于是就构建函数$p(\\vec x)=h((\\vec{\\omega})^T\\cdot\\vec x+b)=h((\\vec{\\theta})^T\\cdot\\vec x)=\\frac{1}{1+e^{-(\\vec{\\omega^T}x+b)}}=\\frac{1}{1+e^{-((\\vec{\\theta})^T\\cdot\\vec x)}}$，($\\vec{\\theta}=(\\theta_0,\\theta_1,\\theta_2,\\theta_3,...,\\theta_n)$，$\\vec x=(1,x_1,x_2,x_3,...,x_n)$假设有$n$个特征，其中$\\theta_0$是常数，即偏移项)与线性回归一样，训练模型就是要找最优的$(\\vec{\\theta})^T$。而这个函数$p(\\vec x)$就是当自变量$\\vec x$是第$i$个数据的$\\vec {x^{(i)}}$时，因变量$p(\\vec{x^{(i)}})$是为A类或B类的概率。\n",
    "### 3）逻辑回归的定义：逻辑回归是用于二分类任务的统计学习模型，通过 Sigmoid 函数将线性回归结果映射到 $[0,1]$ 区间，输出事件发生的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7b7fe-158f-4c4d-8bb1-c5a7467abb9c",
   "metadata": {},
   "source": [
    "# 2.逻辑回归的数学计算\n",
    "### 1)逻辑回归的概率函数的特征：设有一个二分类问题，将其中一类设为$1$，另一类设为$0$，即$y^{(i)}=\\left\\{0,1\\right\\}$，设分类为$1$的概率为$p(y=1)$，在1.2）中描述，分类为A或B的概率为$p(\\vec x)=\\frac{1}{1+e^{-((\\vec{\\theta})^T\\cdot\\vec x)}}$，现假设A类为$1$，B类为$0$，那么，$p(y=1|\\vec x,\\vec\\theta)=\\frac{1}{1+e^{-((\\vec{\\theta})^T\\cdot\\vec x)}}=\\frac{e^{(\\vec{\\theta})^T\\cdot\\vec x}}{1+e^{(\\vec{\\theta})^T\\cdot\\vec x}}$（其中$p(y=1|\\vec x,\\vec\\theta)$的意思是给定$\\vec x$和$\\vec\\theta$的条件下，$y=1$的概率），显然，$p(y=0|\\vec x,\\vec\\theta)=1-p(y=1|\\vec x,\\vec\\theta)=\\frac{1}{1+e^{(\\vec{\\theta})^T\\cdot\\vec x}}$。于是$ln(\\frac{p(y=1|\\vec x,\\vec\\theta)}{p(y=0|\\vec x,\\vec\\theta)})=(\\vec{\\theta})^T\\cdot\\vec x$\n",
    "### 2）最大似然估计：\n",
    "#### a.似然函数的定义：似然函数是观测到数据后，描述模型参数与数据匹配程度的函数，本质是 “基于已有数据，参数的可能性分布”。\n",
    "#### b.似然函数与条件概率的关系：条件概率$p(D|\\theta)$是在参数$\\theta$的条件下，$D$发生的概率，即$\\theta$是固定的，而似然函数$L(\\theta|D)$是固定了事件$D$，衡量不同参数$\\theta$使$D$发生的概率，于是，在数学上他们的关系是：若$D$有$p$个样本，$D=\\left\\{x_1,x_2,x_3,...,x_p\\right\\}$，那么$L(\\theta|D)=L(\\theta|x_i)=\\prod_{i=1}^{p}P(x_i|\\theta)$，举个例子：假设有一枚硬币正面的概率为$\\theta$，有三个样本$\\left\\{正,正,反\\right\\}$，那么他的似然函数是：$L(\\theta|x_i)=\\theta^2(1-\\theta)$。当它的概率不是离散的，而是连续的，如满足正态分布，泊松分布等的时候，似然函数仍然成立。\n",
    "#### c.最大似然估计：对于硬币的例子：它的样本是：$\\left\\{正,正,反\\right\\}$，现在想要估计最优的概率$\\theta$使得它满足现有的样本，那么显然观察这三个样本，$\\theta$贴近于$\\frac{2}{3}$因为三个样本中有两个是正的，一个是反的。对于似然函数：$L(\\theta|x_i)=\\theta^2(1-\\theta)$（$\\theta\\in[0,1]$），$\\frac{dL(\\theta|x_i)}{d\\theta}=2\\theta-3\\theta^2=\\theta(2-3\\theta)$，显然，这个函数在$\\theta\\in[0,\\frac{2}{3}]$单调递增，在$\\theta\\in[\\frac{2}{3},1]$是单调递减的，这个函数在$\\theta$的定义域范围里最大值是当$\\theta=\\frac{2}{3}$之时，恰好满足刚刚的推断$\\theta$贴近于$\\frac{2}{3}$时满足样本。于是最优的概率$\\theta$就是似然函数的最大值点。\n",
    "### 3）逻辑回归的似然函数：对于逻辑回归的问题，假设有$m$个数据（样本），每个数据有$n$个特征，每个样本分类为$0$或$1$，逻辑回归的目标是寻找最优的$(\\vec\\theta)^T$使得模型最优即很好地做分类。对于逻辑回归的似然函数的第$i$个样本的条件概率$P(X_i|\\theta)=[p(y^{(i)}=1|\\vec\\theta,\\vec x)]^{y^{(i)}}$（若这个样本分类为$1$）;$P(X_i|\\theta)=[p(y^{(i)}=0|\\vec\\theta,\\vec x)]^{1-y^{(i)}}=[1-p(y^{(i)}=1|\\vec\\theta,\\vec x)]^{1-y^{(i)}}$（若这个样本分类为$0$）（其中$X_i$是指第$i$个样本，既包括$y^{(i)}$，也包括$\\vec x$）故逻辑回归的似然函数为：$L(\\vec\\theta)=\\prod_{i=1}^{m}[p(y^{(i)}=1|\\vec\\theta,\\vec x)]^{y^{(i)}}[p(y^{(i)}=0|\\vec\\theta,\\vec x)]^{1-y^{(i)}}=\\prod_{i=1}^{m}[p(y^{(i)}=1|\\vec\\theta,\\vec x)]^{y^{(i)}}[1-p(y^{(i)}=1|\\vec\\theta,\\vec x)]^{1-y^{(i)}}$。这个似然函数刻画为：参数$(\\vec\\theta)^T$，对于现有分类样本出现的概率。比如，有五个数据（样本）$\\vec x=(x_1,x_2,x_3,x_4,x_5)$，$y^{(i)}=\\left\\{0,1,1,0,1\\right\\}$，他们的分类概率分别是$\\left\\{0.2,0.75,0.8,0.1,0.9\\right\\}$（注意这个分类概率是对第$i$个样本来说的，如第一个样本分类为$0$，由$p(\\vec x)$计算得它分类为$1$的概率为$0.2$，分类为$0$的概率为$0.8$）,那么似然函数就是描述使用$(\\vec\\theta)^T$得到的这个样本概率$\\left\\{0,1,1,0,1\\right\\}$的概率，那就是：$L(\\vec\\theta)=(1-0.2)\\times0.75\\times0.8\\times(1-0.1)\\times0.9=0.3888$，与硬币的例子相似。\n",
    "### 4）逻辑回归的目标函数：想要最优化$(\\vec\\theta)^T$，就需要最大化其似然函数，即最大化$L(\\vec\\theta)=\\prod_{i=1}^{m}[p(y^{(i)}=1|\\vec\\theta,\\vec{x^{(i)}})]^{y^{(i)}}[1-p(y^{(i)}=1|\\vec\\theta,\\vec{x^{(i)}})]^{1-y^{(i)}}$，为了简化计算最大化$L(\\vec\\theta)$等价于最大化$ln(L(\\vec\\theta))$因为$L(\\vec\\theta)\\in(0,1)$（似然函数刻画的是概率，其范围理应是$[0,1]$，在正常情况下，$L(\\vec\\theta)$不太可能是$0$或$1$，因为当有一个样本概率为$0$，那么$1-0=1$，同样，只有当所有条件概率均为$1$的时候，$L(\\vec\\theta)$才会是$1$），而$ln(x)$在$(0,1)$是单调递增的。令$f(\\vec\\theta)=ln(L(\\vec\\theta))=\\sum_{i=1}^{m}(y^{(i)}ln(p(y^{(i)}=1|\\vec\\theta,\\vec {x^{(i)}}))+(1-y^{(i)})ln(1-p(y^{(i)}=1|\\vec\\theta,\\vec{x^{(i)}})))=\\textcolor{red}{\\sum_{i=1}^{m}(y^{(i)}ln(\\sigma((\\vec\\theta)^T\\cdot\\vec{x^{(i)}}))+(1-y^{(i)})ln(1-\\sigma((\\vec\\theta)^T\\cdot\\vec{x^{(i)}})))}$（$\\sigma(x)=\\frac{1}{1+e^{-x}}$），如果想要化简它，就是：$\\sum_{i=1}^{m}(y^{(i)}ln(\\frac{e^{(\\vec{\\theta})^T\\cdot\\vec{x^{(i)}}}}{1+e^{(\\vec{\\theta})^T\\cdot\\vec{x^{(i)}}}})+(1-y^{(i)})ln(\\frac{1}{1+e^{(\\vec{\\theta})^T\\cdot\\vec{x^{(i)}}}}))=\\sum_{i=1}^{m}(y^{(i)}[(\\vec\\theta)^T\\cdot\\vec{x^{(i)}}-ln(1+e^{(\\vec{\\theta})^T\\cdot\\vec{x^{(i)}}})]+(1-y^{(i)})[-ln(1+e^{(\\vec{\\theta})^T\\cdot\\vec{x^{(i)}}})])=\\sum_{i=1}^{m}(y^{(i)}(\\vec\\theta)^T\\cdot\\vec{x^{(i)}}-ln(1+e^{(\\vec{\\theta})^T\\cdot\\vec{x^{(i)}}}))$，于是可得它的损失函数$\\textcolor{red}{J(\\vec\\theta)=-\\frac{1}{m}f(\\vec\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}ln(\\sigma((\\vec\\theta)^T\\cdot\\vec{x^{(i)}}))+(1-y^{(i)})ln(1-\\sigma((\\vec\\theta)^T\\cdot\\vec{x^{(i)}})))}$（$i$是指第$i$个数据），也可以写成$J(\\theta_0,\\theta_1,\\theta_2,\\theta_3,...,\\theta_n)=-\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}ln(\\sigma(\\theta_0+\\theta_1x_1^{(i)}+\\theta_2x_2^{(i)}+...+\\theta_nx_n^{(i)}))+(1-y^{(i)})ln(1-\\sigma(\\theta_0+\\theta_1x_1^{(i)}+\\theta_2x_2^{(i)}+...+\\theta_nx_n^{(i)})))$，称其为交叉熵损失。\n",
    "### 5）使用梯度下降法求解最优：想要最大化似然函数等价于最小化损失函数（因为损失函数是：$J(\\vec\\theta)=-\\frac{1}{m}f(\\vec\\theta)$，而$m>0$）。于是，优化目标为：$\\theta^*=argmin_{\\vec\\theta} J(\\vec\\theta)$（其中$argmin_{\\vec\\theta} J(\\vec\\theta)$是使得函数$J(\\vec\\theta)$最小时，自变量$\\vec\\theta$的取值或取值集合）。对于$\\sigma(x)=\\frac{1}{1+e^{-x}}$，$\\sigma^{'}(x)=\\frac{d\\sigma(x)}{dx}=\\frac{e^{-x}}{(1+e^{-x})^2}$，$\\frac{\\sigma^{'}(x)}{\\sigma(x)}=\\frac{e^{-x}}{1+e^{-x}}=1-\\sigma(x)$，$\\frac{-\\sigma^{'}(x)}{1-\\sigma(x)}=\\frac{-e^x}{1+e^x}=-\\sigma(x)$，按照梯度下降法，先求$\\frac{\\partial J(\\theta_0,\\theta_1,\\theta_2,..,\\theta_n)}{\\partial\\theta_j}=-\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\frac{\\sigma^{'}(\\theta_0+\\theta_1x_1^{(i)}+...+\\theta_nx_n^{(i)})}{\\sigma(\\theta_0+\\theta_1x_1^{(i)}+...+\\theta_nx_n^{(i)})}x^{(i)}_j+(1-y^{(i)})\\frac{-\\sigma^{'}(\\theta_0+\\theta_1x_1^{(i)}+...+\\theta_nx_n^{(i)})}{1-\\sigma(\\theta_0+\\theta_1x_1^{(i)}+...+\\theta_nx_n^{(i)})}x_j^{(i)})=-\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}(1-\\sigma(\\theta_0+\\theta_1x_1^{(i)}+...+\\theta_nx_n^{(i)}))x_j^{(i)}+(1-y^{(i)})(-\\sigma(\\theta_0+\\theta_1x_1^{(i)}+...+\\theta_nx_n^{(i)})))=\\frac{1}{m}\\sum_{i=1}^{m}(\\sigma(\\theta_0+\\theta_1x_1^{(i)}+...+\\theta_nx_n^{(i)})-y^{(i)})x_j^{(i)}$，将其写成向量的形式就是：$\\textcolor{red}{\\frac{\\partial J(\\vec\\theta)}{\\partial\\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}(\\sigma((\\vec\\theta)^T\\cdot\\vec{x^{(i)}})-y^{(i)})x_j^{(i)}}$（注意，$x_0^{(i)}=1$）。与线性回归相似，使用梯度下降法更新$\\vec\\theta$，$\\theta_j^{'}=\\theta_j-\\alpha\\cdot\\frac{\\partial J(\\theta)}{\\partial\\theta_j}$（$\\alpha$是学习率且$\\alpha>0$），所有的$\\theta_j$同时更新，最终找到最优的$\\vec\\theta$。同样的，与线性回归一样，逻辑回归也有正则化，L1正则化：$J(\\vec\\theta)=\\frac{\\partial J(\\vec\\theta)}{\\partial\\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}(\\sigma((\\vec\\theta)^T\\cdot\\vec{x^{(i)}})-y^{(i)})x_j^{(i)}+\\lambda||\\vec\\theta||_1$（$||\\vec\\theta||_1=\\sum_{j=1}^{n}|\\theta_j|$，它是1-范数）；L2正则化：$J(\\vec\\theta)=\\frac{\\partial J(\\vec\\theta)}{\\partial\\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}(\\sigma((\\vec\\theta)^T\\cdot\\vec{x^{(i)}})-y^{(i)})x_j^{(i)}+\\frac{\\lambda}{2m}||\\vec\\theta||_2^2$（$||\\vec\\theta||_2=\\sqrt{\\sum_{j=1}^{n}(\\theta_j)^2}$，它是2-范数）（无论是哪一个正则化，其正则项的$j$都是从$1$开始的）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f6690f-3f6f-41c8-a634-0c2280effe78",
   "metadata": {},
   "source": [
    "# 3. 多分类问题\n",
    "### 1）多分类的定义：给定输入特征$\\vec x$，预测其所属类别 $(y^{(i)}\\in\\left\\{1, 2, ..., k\\right\\})$且$k\\ge3$，其中各类别互斥（一个样本仅属一类）且穷尽（所有样本必属某一类）。\n",
    "### 2）多分类问题的引入：逻辑回归使用的sigmoid函数，只能解决二分类问题，当涉及到多分类，无法使用它去解决。对于多分类问题，对于类别而言，其条件概率为：$p(y=c|\\vec x,\\vec{\\theta_c})$（其中$\\vec{\\theta_c}$的意思是对于每一个类别$c$，都有其对应的$\\vec\\theta$，$\\vec{\\theta_c}=(\\theta_{c0},\\theta_{c1},\\theta_{c2},...,\\theta_{cn})^T$；$\\vec x=(1,x_1,x_2,...,x_n)^T$假设有$n$个特征），显然，必定满足$\\sum_{c=1}^{k}p(y=c|\\vec x,\\vec{\\theta_c})=1$且$0<p(y=c|\\vec x,\\vec{\\theta_c})<1$，每一个类别$c$，都有一个$\\vec\\theta_c$，所以每一个类别$c$，都有一个线性预测值：$z_c=(\\vec{\\theta_c})^T\\cdot\\vec x$\n",
    "### 3）softmax函数与多分类问题：softmax函数用于将一个实数向量映射为概率分布的函数，写为：$\\phi(\\vec Z)_i=\\frac{e^{z_i}}{\\sum_{i=1}^{p}e^{z_i}}$（$\\vec Z=(z_1,z_2,z_3,...,z_p)^T$）例如，有一个向量$\\vec Z=[1,2,3]^T$，其softmax函数的映射值为：$\\phi(\\vec Z)_1=\\frac{e^1}{e^1+e^2+e^3}=0.09003057317038046$；$\\phi(\\vec Z)_2=\\frac{e^2}{e^1+e^2+e^3}=0.24472847105479767$，$\\phi(\\vec Z)_3=\\frac{e^2}{e^1+e^2+e^3}0.6652409557748219$，有其函数特性可知$\\sum_{i=1}^{p}\\phi(\\vec Z)_i=1$且$0<\\phi(\\vec Z)_i<1$，所以它适合当多分类问题的映射函数，将每一个类别$c$，的线性预测值形成一个向量$\\vec Z=(z_1,z_2,z_3,...,z_k)^T$（有$k$个分类），于是就得到了$p(y=c|\\vec x,\\vec{\\theta_c})=\\phi(\\vec Z)_c=\\frac{e^{z_c}}{\\sum_{c=1}^{k}e^{z_c}}=\\frac{e^{(\\vec{\\theta_c})^T\\cdot\\vec x}}{\\sum_{c=1}^{k}e^{(\\vec{\\theta_c})^T\\cdot\\vec x}}$。\n",
    "### 4）多分类的最大似然：对于数据（样本），有$m$个，即：$\\left\\{\\vec{x^{(1)}},y^{(1)}\\right\\},\\left\\{\\vec{x^{(2)}},y^{(2)}\\right\\},...,\\left\\{\\vec{x^{(m)}},y^{(m)}\\right\\}$，其中$y^{(i)}\\in\\left\\{1, 2, ..., k\\right\\}$（$k\\ge3$）。首先，使用$\\vec\\Theta$表示所有的$\\vec\\theta$，$\\vec\\Theta=(\\vec{\\theta_1},\\vec{\\theta_2},\\vec{\\theta_3},...,\\vec{\\theta_k})^T$，其中$\\vec{\\theta_c}=(\\theta_{c0},\\theta_{c1},\\theta_{c2},...,\\theta_{cn})^T$其表示为第$c$个分类对应的$\\theta_0$到$\\theta_n$。然后，构造多分类的似然函数，与逻辑回归（二分类）的似然函数类似，其刻画的是在参数$\\vec\\Theta$下得到特定样本$\\left\\{y^{(1)},y^{(2)},y^{(3)},...,y^{(m)}\\right\\}$的概率，于是就有：$L(\\vec\\Theta)=\\prod_{i=1}^{m}p(y=y^{(i)}|\\vec{x^{(i)}},\\vec{\\Theta})=\\prod_{i=1}^{m}\\frac{e^{(\\vec{\\theta_q})^T\\cdot\\vec{x^{(i)}}}}{\\sum_{c=1}^{k}e^{(\\vec{\\theta_c})^T\\cdot\\vec{x^{(i)}}}}$（$q=y^{(i)}$，后面皆用$q$表示$y^{(i)}$，指的是第$i$个样本的$y^{(i)}$）。要最大化$L(\\vec\\Theta)$等价于最大化$ln(L(\\vec\\Theta))=\\sum_{i=1}^{m}((\\vec{\\theta_q})^T\\cdot\\vec{x^{(i)}}-ln(\\sum_{c=1}^{k}e^{(\\vec{\\theta_c})^T\\cdot\\vec{x^{(i)}}}))$。\n",
    "### 5）多分类的损失函数及梯度下降：令$f(\\vec\\Theta)=ln(L(\\vec\\Theta))$，于是多分类的损失函数为：$J(\\vec\\Theta)=-\\frac{1}{m}f(\\vec\\Theta)=\\textcolor{red}{-\\frac{1}{m}\\sum_{i=1}^{m}((\\vec{\\theta_q})^T\\cdot\\vec{x^{(i)}}-ln(\\sum_{c=1}^{k}e^{(\\vec{\\theta_c})^T\\cdot\\vec{x^{(i)}}}))}$，其一般形式为：$J(\\vec\\Theta)=-\\frac{1}{m}\\sum_{i=1}^{m}((\\theta_{q0}+\\theta_{q1}x_1^{(i)}+\\theta_{q2}x_2^{(i)}+...+\\theta_{qn}x_n^{(i)})-ln(\\sum_{c=1}^{k}e^{(\\theta_{c0}+\\theta_{c1}x_1^{(i)}+\\theta_{c2}x_2^{(i)}+...+\\theta_{cn}x_n^{(i)})}))$，称它为多分类的交叉损失熵。同样的，要最小化损失函数。目标：要更新每个类别$c$的$\\vec{\\theta_c}$。于是，$J(\\vec\\Theta)$对$\\vec{\\theta_c}$求偏导，得到的结果为：$(\\frac{\\partial J(\\vec\\Theta)}{\\partial\\theta_{c0}},\\frac{\\partial J(\\vec\\Theta)}{\\partial\\theta_{c1}},\\frac{\\partial J(\\vec\\Theta)}{\\partial\\theta_{c2}},...,\\frac{\\partial J(\\vec\\Theta)}{\\partial\\theta_{cn}})^T$（依次求解$c$，从$1$到$k$，就可以得到每个类别的$\\vec{\\theta_c}$），要求每一个特征$j$对应的$\\frac{\\partial J(\\vec\\Theta)}{\\partial\\theta_{cj}}$，先看第一项:$\\theta_{q0}+\\theta_{q1}x_1^{(i)}+\\theta_{q2}x_2^{(i)}+...+\\theta_{qn}x_n^{(i)}$，当且仅当$q=y^{(i)}=c$的时候，它对$\\theta_{cj}$偏导为$x_j^{(i)}$；再看第二项：$ln(\\sum_{c=1}^{k}e^{(\\theta_{c0}+\\theta_{c1}x_1^{(i)}+\\theta_{c2}x_2^{(i)}+...+\\theta_{cn}x_n^{(i)})})$，它对于$\\theta_{cj}$的偏导为：$\\frac{x_j^{(i)}e^{(\\theta_{c0}+\\theta_{c1}x_1^{(i)}+\\theta_{c2}x_2^{(i)}+...+\\theta_{cn}x_n^{(i)})}}{\\sum_{c=1}^{k}e^{(\\theta_{c0}+\\theta_{c1}x_1^{(i)}+\\theta_{c2}x_2^{(i)}+...+\\theta_{cn}x_n^{(i)})}}=p(y=c|\\vec x,\\vec{\\Theta})=\\phi(\\vec Z)_c$（$\\vec Z=(z_1,z_2,z_3,...,z_k)^T$，$z_c$表示第$c$个分类的线性预测，$z_c=(\\vec{\\theta_c})^T\\cdot\\vec x=\\theta_{c0}+\\theta_{c1}x_1+\\theta_{c2}x_2+...+\\theta_{cn}x_n$）。于是，$\\frac{\\partial J(\\vec\\Theta)}{\\partial\\theta_{cj}}=\\textcolor{red}{\\frac{1}{m}\\sum_{i=1}^{m}(p(y=c|\\vec x,\\vec{\\Theta})-I(y^{(i)}=c))x_j^{(i)}=\\frac{1}{m}\\sum_{i=1}^{m}(\\phi(\\vec Z)_c-I(y^{(i)}=c))x_j^{(i)}}$（$x_0^{(i)}=1$）（其中$I(y^{(i)}=c)$含义为当满足$y^{(i)}=c$时$I(y^{(i)}=c)=1$，否则$I(y^{(i)}=c)=0$，$\\phi(\\vec Z)_c=\\frac{e^{z_c}}{\\sum_{c=1}^{k}e^{z_c}}$）。于是关于$J(\\vec\\Theta)$对$\\vec{\\theta_c}$求偏导的结果为：$\\frac{\\partial J(\\vec\\Theta)}{\\partial\\vec{\\theta_c}}=\\frac{1}{m}\\sum_{i=1}^{m}(\\phi(\\vec Z)_c-I(y^{(i)}=c))\\vec{x^{(i)}}$。梯度下降法更新$\\vec\\theta_c$，$(\\vec\\theta_c)^{'}=\\vec\\theta_c-\\alpha\\cdot\\frac{\\partial J(\\vec\\Theta)}{\\partial\\vec\\theta_c}$。\n",
    "### 6）关于多分类问题的L1和L2正则化：与逻辑回归一样，多分类问题亦有正则化，L1正则化：$J(\\vec\\Theta)=-\\frac{1}{m}\\sum_{i=1}^{m}((\\vec{\\theta_q})^T\\cdot\\vec{x^{(i)}}-ln(\\sum_{c=1}^{k}e^{(\\vec{\\theta_c})^T\\cdot\\vec{x^{(i)}}}))+\\lambda\\sum_{c=1}^{k}\\sum_{j=1}^{n}|\\theta_{cj}|$；L2正则化：$J(\\vec\\Theta)=-\\frac{1}{m}\\sum_{i=1}^{m}((\\vec{\\theta_q})^T\\cdot\\vec{x^{(i)}}-ln(\\sum_{c=1}^{k}e^{(\\vec{\\theta_c})^T\\cdot\\vec{x^{(i)}}}))+\\frac{\\lambda}{2m}\\sum_{c=1}^{k}\\sum_{j=1}^{n}\\theta_{cj}^2$（无论是哪一个正则化，其正则项的$j$都是从$1$开始的）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
